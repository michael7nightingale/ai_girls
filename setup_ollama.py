#!/usr/bin/env python3
"""
–°–∫—Ä–∏–ø—Ç –¥–ª—è —É—Å—Ç–∞–Ω–æ–≤–∫–∏ –∏ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ Ollama –¥–ª—è AI Girls
"""

import os
import sys
import subprocess
import platform
import requests
import time
from pathlib import Path


def check_ollama_installed() -> bool:
    """–ü—Ä–æ–≤–µ—Ä–∫–∞ —É—Å—Ç–∞–Ω–æ–≤–∫–∏ Ollama"""
    try:
        result = subprocess.run(['ollama', '--version'], 
                              capture_output=True, text=True)
        return result.returncode == 0
    except FileNotFoundError:
        return False


def install_ollama():
    """–£—Å—Ç–∞–Ω–æ–≤–∫–∞ Ollama"""
    system = platform.system().lower()
    
    print("üöÄ –£—Å—Ç–∞–Ω–æ–≤–∫–∞ Ollama...")
    
    if system == "linux":
        install_ollama_linux()
    elif system == "darwin":  # macOS
        install_ollama_macos()
    elif system == "windows":
        install_ollama_windows()
    else:
        print(f"‚ùå –ù–µ–ø–æ–¥–¥–µ—Ä–∂–∏–≤–∞–µ–º–∞—è –æ–ø–µ—Ä–∞—Ü–∏–æ–Ω–Ω–∞—è —Å–∏—Å—Ç–µ–º–∞: {system}")
        return False
    
    return True


def install_ollama_linux():
    """–£—Å—Ç–∞–Ω–æ–≤–∫–∞ Ollama –Ω–∞ Linux"""
    try:
        # –£—Å—Ç–∞–Ω–æ–≤–∫–∞ —á–µ—Ä–µ–∑ curl
        install_script = """
        curl -fsSL https://ollama.ai/install.sh | sh
        """
        
        print("üì• –ó–∞–≥—Ä—É–∑–∫–∞ –∏ —É—Å—Ç–∞–Ω–æ–≤–∫–∞ Ollama...")
        result = subprocess.run(install_script, shell=True, capture_output=True, text=True)
        
        if result.returncode == 0:
            print("‚úÖ Ollama —É—Å–ø–µ—à–Ω–æ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω!")
        else:
            print(f"‚ùå –û—à–∏–±–∫–∞ —É—Å—Ç–∞–Ω–æ–≤–∫–∏: {result.stderr}")
            return False
            
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ —É—Å—Ç–∞–Ω–æ–≤–∫–∏ Ollama: {e}")
        return False


def install_ollama_macos():
    """–£—Å—Ç–∞–Ω–æ–≤–∫–∞ Ollama –Ω–∞ macOS"""
    try:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ Homebrew
        result = subprocess.run(['which', 'brew'], capture_output=True)
        if result.returncode != 0:
            print("‚ùå Homebrew –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω. –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ Homebrew: https://brew.sh")
            return False
        
        # –£—Å—Ç–∞–Ω–æ–≤–∫–∞ —á–µ—Ä–µ–∑ Homebrew
        print("üì• –£—Å—Ç–∞–Ω–æ–≤–∫–∞ Ollama —á–µ—Ä–µ–∑ Homebrew...")
        result = subprocess.run(['brew', 'install', 'ollama'], capture_output=True, text=True)
        
        if result.returncode == 0:
            print("‚úÖ Ollama —É—Å–ø–µ—à–Ω–æ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω!")
        else:
            print(f"‚ùå –û—à–∏–±–∫–∞ —É—Å—Ç–∞–Ω–æ–≤–∫–∏: {result.stderr}")
            return False
            
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ —É—Å—Ç–∞–Ω–æ–≤–∫–∏ Ollama: {e}")
        return False


def install_ollama_windows():
    """–£—Å—Ç–∞–Ω–æ–≤–∫–∞ Ollama –Ω–∞ Windows"""
    print("üì• –î–ª—è Windows —Å–∫–∞—á–∞–π—Ç–µ Ollama —Å https://ollama.ai/download")
    print("–ü–æ—Å–ª–µ —É—Å—Ç–∞–Ω–æ–≤–∫–∏ –∑–∞–ø—É—Å—Ç–∏—Ç–µ —ç—Ç–æ—Ç —Å–∫—Ä–∏–ø—Ç —Å–Ω–æ–≤–∞")
    return False


def start_ollama_service():
    """–ó–∞–ø—É—Å–∫ —Å–µ—Ä–≤–∏—Å–∞ Ollama"""
    try:
        print("üîÑ –ó–∞–ø—É—Å–∫ —Å–µ—Ä–≤–∏—Å–∞ Ollama...")
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º, –∑–∞–ø—É—â–µ–Ω –ª–∏ —É–∂–µ —Å–µ—Ä–≤–∏—Å
        try:
            response = requests.get("http://localhost:11434/api/tags", timeout=5)
            if response.status_code == 200:
                print("‚úÖ –°–µ—Ä–≤–∏—Å Ollama —É–∂–µ –∑–∞–ø—É—â–µ–Ω")
                return True
        except:
            pass
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º —Å–µ—Ä–≤–∏—Å
        if platform.system().lower() == "windows":
            subprocess.Popen(['ollama', 'serve'], 
                           creationflags=subprocess.CREATE_NEW_CONSOLE)
        else:
            subprocess.Popen(['ollama', 'serve'])
        
        # –ñ–¥–µ–º –∑–∞–ø—É—Å–∫–∞ —Å–µ—Ä–≤–∏—Å–∞
        print("‚è≥ –û–∂–∏–¥–∞–Ω–∏–µ –∑–∞–ø—É—Å–∫–∞ —Å–µ—Ä–≤–∏—Å–∞...")
        for i in range(30):  # –ñ–¥–µ–º –¥–æ 30 —Å–µ–∫—É–Ω–¥
            try:
                response = requests.get("http://localhost:11434/api/tags", timeout=5)
                if response.status_code == 200:
                    print("‚úÖ –°–µ—Ä–≤–∏—Å Ollama —É—Å–ø–µ—à–Ω–æ –∑–∞–ø—É—â–µ–Ω!")
                    return True
            except:
                pass
            time.sleep(1)
        
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–ø—É—Å—Ç–∏—Ç—å —Å–µ—Ä–≤–∏—Å Ollama")
        return False
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–ø—É—Å–∫–∞ —Å–µ—Ä–≤–∏—Å–∞: {e}")
        return False


def download_models():
    """–ó–∞–≥—Ä—É–∑–∫–∞ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã—Ö –º–æ–¥–µ–ª–µ–π"""
    recommended_models = [
        "llama2",
        "mistral",
        "neural-chat"
    ]
    
    print("üì• –ó–∞–≥—Ä—É–∑–∫–∞ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã—Ö –º–æ–¥–µ–ª–µ–π...")
    
    for model in recommended_models:
        print(f"üì¶ –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ {model}...")
        try:
            result = subprocess.run(['ollama', 'pull', model], 
                                  capture_output=True, text=True)
            if result.returncode == 0:
                print(f"‚úÖ –ú–æ–¥–µ–ª—å {model} –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
            else:
                print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ {model}: {result.stderr}")
        except Exception as e:
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ {model}: {e}")


def test_ollama():
    """–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ Ollama"""
    print("üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ Ollama...")
    
    try:
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å API
        response = requests.get("http://localhost:11434/api/tags")
        if response.status_code != 200:
            print("‚ùå API Ollama –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω")
            return False
        
        # –ü—Ä–æ–≤–µ—Ä—è–µ–º –Ω–∞–ª–∏—á–∏–µ –º–æ–¥–µ–ª–µ–π
        models = response.json().get("models", [])
        if not models:
            print("‚ö†Ô∏è  –ú–æ–¥–µ–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã. –ó–∞–ø—É—Å—Ç–∏—Ç–µ download_models()")
            return False
        
        print(f"‚úÖ –ù–∞–π–¥–µ–Ω–æ {len(models)} –º–æ–¥–µ–ª–µ–π:")
        for model in models[:5]:  # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –ø–µ—Ä–≤—ã–µ 5
            print(f"  - {model['name']}")
        
        # –¢–µ—Å—Ç–∏—Ä—É–µ–º –≥–µ–Ω–µ—Ä–∞—Ü–∏—é
        print("üß™ –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏...")
        test_prompt = {
            "model": "llama2",
            "prompt": "–ü—Ä–∏–≤–µ—Ç! –ö–∞–∫ –¥–µ–ª–∞?",
            "stream": False
        }
        
        response = requests.post("http://localhost:11434/api/generate", 
                               json=test_prompt, timeout=30)
        
        if response.status_code == 200:
            result = response.json()
            if "response" in result:
                print("‚úÖ –ì–µ–Ω–µ—Ä–∞—Ü–∏—è —Ä–∞–±–æ—Ç–∞–µ—Ç!")
                return True
        
        print("‚ùå –û—à–∏–±–∫–∞ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏")
        return False
        
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è: {e}")
        return False


def setup_ollama_config():
    """–ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ Ollama"""
    print("‚öôÔ∏è  –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏...")
    
    # –°–æ–∑–¥–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–æ–Ω–Ω—ã–π —Ñ–∞–π–ª –¥–ª—è AI Girls
    config_dir = Path.home() / ".ollama"
    config_dir.mkdir(exist_ok=True)
    
    config_file = config_dir / "ai_girls_config.json"
    config = {
        "default_model": "llama2",
        "temperature": 0.85,
        "top_p": 0.92,
        "max_tokens": 250,
        "repeat_penalty": 1.15,
        "top_k": 40
    }
    
    try:
        import json
        with open(config_file, 'w') as f:
            json.dump(config, f, indent=2)
        print("‚úÖ –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞")
    except Exception as e:
        print(f"‚ö†Ô∏è  –û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏: {e}")


def main():
    """–ì–ª–∞–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    print("ü§ñ AI Girls - –ù–∞—Å—Ç—Ä–æ–π–∫–∞ Ollama")
    print("=" * 40)
    
    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —É—Å—Ç–∞–Ω–æ–≤–∫—É
    if not check_ollama_installed():
        print("üì• Ollama –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
        if not install_ollama():
            return
    else:
        print("‚úÖ Ollama —É–∂–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω")
    
    # –ó–∞–ø—É—Å–∫–∞–µ–º —Å–µ—Ä–≤–∏—Å
    if not start_ollama_service():
        print("‚ùå –ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–ø—É—Å—Ç–∏—Ç—å —Å–µ—Ä–≤–∏—Å")
        return
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª–∏
    download_models()
    
    # –ù–∞—Å—Ç—Ä–∞–∏–≤–∞–µ–º –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é
    setup_ollama_config()
    
    # –¢–µ—Å—Ç–∏—Ä—É–µ–º
    if test_ollama():
        print("\nüéâ Ollama —É—Å–ø–µ—à–Ω–æ –Ω–∞—Å—Ç—Ä–æ–µ–Ω –¥–ª—è AI Girls!")
        print("\nüìã –°–ª–µ–¥—É—é—â–∏–µ —à–∞–≥–∏:")
        print("1. –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –≤ .env —Ñ–∞–π–ª–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ: USE_OLLAMA=true")
        print("2. –ó–∞–ø—É—Å—Ç–∏—Ç–µ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏–µ: python run.py")
        print("3. –ù–∞—Å–ª–∞–∂–¥–∞–π—Ç–µ—Å—å –æ–±—â–µ–Ω–∏–µ–º —Å AI –ø–µ—Ä—Å–æ–Ω–∞–∂–∞–º–∏!")
    else:
        print("\n‚ùå –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –Ω–µ –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –æ—à–∏–±–∫–∏ –≤—ã—à–µ.")


if __name__ == "__main__":
    main()
