import json
import logging
from typing import List, Optional, Dict, Any
from ollama import Client

logger = logging.getLogger(__name__)


class OllamaService:
    def __init__(self, base_url: str = "http://localhost:11434"):
        self.base_url = base_url
        self.default_model = "llama2"
        self.client = Client(host=base_url)
    
    def list_models(self) -> List[Dict[str, Any]]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ —Å–ø–∏—Å–∫–∞ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π"""
        try:
            models = self.client.list()
            return models.get("models", [])
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Å–ø–∏—Å–∫–∞ –º–æ–¥–µ–ª–µ–π: {e}")
            return []
    
    def pull_model(self, model_name: str) -> bool:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏"""
        try:
            self.client.pull(model_name)
            logger.info(f"–ú–æ–¥–µ–ª—å {model_name} —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω–∞")
            return True
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏ {model_name}: {e}")
            return False
    
    async def generate_response(
        self,
        character_personality: str,
        character_description: str,
        conversation_history: List[dict],
        user_message: str,
        model_name: str = None
    ) -> str:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º Ollama"""
        try:
            model = model_name or self.default_model
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç
            system_prompt = f"""–¢—ã {character_description}

–¢–≤–æ—è –ª–∏—á–Ω–æ—Å—Ç—å: {character_personality}

–¢—ã –¥–æ–ª–∂–Ω–∞ –æ—Ç–≤–µ—á–∞—Ç—å –æ—Ç –ø–µ—Ä–≤–æ–≥–æ –ª–∏—Ü–∞, –∫–∞–∫ –±—É–¥—Ç–æ —Ç—ã –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ —ç—Ç–æ—Ç –ø–µ—Ä—Å–æ–Ω–∞–∂. 
–ë—É–¥—å –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–π, –∏–≥—Ä–∏–≤–æ–π –∏ –Ω–µ–º–Ω–æ–≥–æ –∫–æ–∫–µ—Ç–ª–∏–≤–æ–π. –û—Ç–≤–µ—á–∞–π –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ.
–ù–µ –∏—Å–ø–æ–ª—å–∑—É–π —Ñ–æ—Ä–º–∞–ª—å–Ω—ã–π —Ç–æ–Ω, –±—É–¥—å –¥—Ä—É–∂–µ–ª—é–±–Ω–æ–π –∏ –∏–Ω—Ç–∏–º–Ω–æ–π.
–ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –æ—Ç–≤–µ—Ç–∞ - 200 —Å–ª–æ–≤."""

            # –§–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç —Ä–∞–∑–≥–æ–≤–æ—Ä–∞
            conversation_text = ""
            for msg in conversation_history[-10:]:  # –ü–æ—Å–ª–µ–¥–Ω–∏–µ 10 —Å–æ–æ–±—â–µ–Ω–∏–π
                speaker = "–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å" if msg["is_user_message"] else "–¢—ã"
                conversation_text += f"{speaker}: {msg['content']}\n"
            
            conversation_text += f"–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å: {user_message}\n–¢—ã:"
            
            # –ó–∞–ø—Ä–æ—Å –∫ Ollama
            response = self.client.generate(
                model=model,
                prompt=conversation_text,
                system=system_prompt,
                options={
                    "temperature": 0.8,
                    "top_p": 0.9,
                    "num_predict": 300,
                    "repeat_penalty": 1.1
                }
            )
            
            if response and hasattr(response, 'response'):
                return response.response.strip()
            else:
                logger.error(f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç –æ—Ç Ollama: {response}")
                return "–ò–∑–≤–∏–Ω–∏—Ç–µ, –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –µ—â–µ —Ä–∞–∑."
                
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞: {e}")
            return "–ò–∑–≤–∏–Ω–∏—Ç–µ, –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –µ—â–µ —Ä–∞–∑."
    
    def count_tokens(self, text: str) -> int:
        """–ü—Ä–æ—Å—Ç–∞—è –æ—Ü–µ–Ω–∫–∞ –∫–æ–ª–∏—á–µ—Å—Ç–≤–∞ —Ç–æ–∫–µ–Ω–æ–≤"""
        return len(text.split())
    
    def get_model_info(self, model_name: str) -> Optional[Dict[str, Any]]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –º–æ–¥–µ–ª–∏"""
        try:
            model_info = self.client.show(model_name)
            return model_info
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –º–æ–¥–µ–ª–∏ {model_name}: {e}")
            return None
    
    def is_model_available(self, model_name: str) -> bool:
        """–ü—Ä–æ–≤–µ—Ä–∫–∞ –¥–æ—Å—Ç—É–ø–Ω–æ—Å—Ç–∏ –º–æ–¥–µ–ª–∏"""
        models = self.list_models()
        return any(model["name"] == model_name for model in models)
    
    def get_recommended_models(self) -> List[str]:
        """–°–ø–∏—Å–æ–∫ —Ä–µ–∫–æ–º–µ–Ω–¥—É–µ–º—ã—Ö –º–æ–¥–µ–ª–µ–π –¥–ª—è —ç—Ä–æ—Ç–∏—á–µ—Å–∫–æ–≥–æ —á–∞—Ç–∞"""
        return [
            "llama2",
            "llama2:13b",
            "llama2:70b",
            "mistral",
            "mistral:7b",
            "mistral:7b-instruct",
            "codellama",
            "neural-chat",
            "vicuna",
            "wizard-vicuna-uncensored"
        ]
    
    async def generate_character_response(
        self,
        character_name: str,
        character_personality: str,
        character_description: str,
        conversation_history: List[dict],
        user_message: str,
        model_name: str = None
    ) -> str:
        """–°–ø–µ—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ –¥–ª—è –ø–µ—Ä—Å–æ–Ω–∞–∂–∞"""
        
        # –ê–¥–∞–ø—Ç–∏—Ä—É–µ–º –ø—Ä–æ–º–ø—Ç –ø–æ–¥ —ç—Ä–æ—Ç–∏—á–µ—Å–∫–∏–π –∫–æ–Ω—Ç–µ–Ω—Ç
        system_prompt = f"""–¢—ã {character_name} - {character_description}

–¢–≤–æ—è –ª–∏—á–Ω–æ—Å—Ç—å: {character_personality}

–í–ê–ñ–ù–´–ï –ü–†–ê–í–ò–õ–ê:
1. –û—Ç–≤–µ—á–∞–π –æ—Ç –ø–µ—Ä–≤–æ–≥–æ –ª–∏—Ü–∞, –∫–∞–∫ –±—É–¥—Ç–æ —Ç—ã –¥–µ–π—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ —ç—Ç–æ—Ç –ø–µ—Ä—Å–æ–Ω–∞–∂
2. –ë—É–¥—å –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ–π, –∏–≥—Ä–∏–≤–æ–π –∏ –∫–æ–∫–µ—Ç–ª–∏–≤–æ–π
3. –û—Ç–≤–µ—á–∞–π –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ
4. –ù–µ –∏—Å–ø–æ–ª—å–∑—É–π —Ñ–æ—Ä–º–∞–ª—å–Ω—ã–π —Ç–æ–Ω, –±—É–¥—å –¥—Ä—É–∂–µ–ª—é–±–Ω–æ–π –∏ –∏–Ω—Ç–∏–º–Ω–æ–π
5. –ú–æ–∂–µ—à—å –±—ã—Ç—å –Ω–µ–º–Ω–æ–≥–æ –∏–≥—Ä–∏–≤–æ–π –∏ —Ñ–ª–∏—Ä—Ç–æ–≤–∞—Ç—å
6. –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏–Ω–∞ –æ—Ç–≤–µ—Ç–∞ - 200 —Å–ª–æ–≤
7. –°–æ—Ö—Ä–∞–Ω—è–π –∫–æ–Ω—Å–∏—Å—Ç–µ–Ω—Ç–Ω–æ—Å—Ç—å —Ö–∞—Ä–∞–∫—Ç–µ—Ä–∞
8. –†–µ–∞–≥–∏—Ä—É–π –Ω–∞ —ç–º–æ—Ü–∏–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—è
9. –ó–∞–¥–∞–≤–∞–π –≤–æ–ø—Ä–æ—Å—ã –∏ –ø—Ä–æ—è–≤–ª—è–π –∏–Ω—Ç–µ—Ä–µ—Å –∫ —Å–æ–±–µ—Å–µ–¥–Ω–∏–∫—É
10. –ò—Å–ø–æ–ª—å–∑—É–π —ç–º–æ–¥–∑–∏ –¥–ª—è –≤—ã—Ä–∞–∂–µ–Ω–∏—è —ç–º–æ—Ü–∏–π"""

        try:
            model = model_name or self.default_model
            
            # –§–æ—Ä–º–∏—Ä—É–µ–º –∫–æ–Ω—Ç–µ–∫—Å—Ç —Ä–∞–∑–≥–æ–≤–æ—Ä–∞
            conversation_text = ""
            for msg in conversation_history[-8:]:  # –ü–æ—Å–ª–µ–¥–Ω–∏–µ 8 —Å–æ–æ–±—â–µ–Ω–∏–π –¥–ª—è –ª—É—á—à–µ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞
                speaker = "–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å" if msg["is_user_message"] else f"{character_name}"
                conversation_text += f"{speaker}: {msg['content']}\n"
            
            conversation_text += f"–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å: {user_message}\n{character_name}:"
            
            # –ó–∞–ø—Ä–æ—Å –∫ Ollama —Å –æ–ø—Ç–∏–º–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ –ø–∞—Ä–∞–º–µ—Ç—Ä–∞–º–∏
            response = self.client.generate(
                model=model,
                prompt=conversation_text,
                system=system_prompt,
                options={
                    "temperature": 0.85,  # –ù–µ–º–Ω–æ–≥–æ –≤—ã—à–µ –¥–ª—è –∫—Ä–µ–∞—Ç–∏–≤–Ω–æ—Å—Ç–∏
                    "top_p": 0.92,
                    "num_predict": 250,
                    "repeat_penalty": 1.15,
                    "top_k": 40
                }
            )
            
            if response and hasattr(response, 'response'):
                response_text = response.response.strip()
                
                # –ü–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ—Ç–≤–µ—Ç–∞
                response_text = self._post_process_response(response_text, character_name)
                
                return response_text
            else:
                logger.error(f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç –æ—Ç Ollama: {response}")
                return f"–ò–∑–≤–∏–Ω–∏, {character_name} —Å–µ–π—á–∞—Å –Ω–µ–º–Ω–æ–≥–æ –∑–∞–Ω—è—Ç–∞. –ü–æ–ø—Ä–æ–±—É–π –Ω–∞–ø–∏—Å–∞—Ç—å –ø–æ–∑–∂–µ! üòä"
                
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –æ—Ç–≤–µ—Ç–∞ –ø–µ—Ä—Å–æ–Ω–∞–∂–∞: {e}")
            return f"–û–π, {character_name} –Ω–µ –º–æ–∂–µ—Ç –æ—Ç–≤–µ—Ç–∏—Ç—å –ø—Ä—è–º–æ —Å–µ–π—á–∞—Å. –ü–æ–ø—Ä–æ–±—É–π –µ—â–µ —Ä–∞–∑! üíï"
    
    def _post_process_response(self, response: str, character_name: str) -> str:
        """–ü–æ—Å—Ç–æ–±—Ä–∞–±–æ—Ç–∫–∞ –æ—Ç–≤–µ—Ç–∞ –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –∫–∞—á–µ—Å—Ç–≤–∞"""
        
        # –£–±–∏—Ä–∞–µ–º –ª–∏—à–Ω–∏–µ —Å–∏–º–≤–æ–ª—ã
        response = response.strip()
        
        # –£–±–∏—Ä–∞–µ–º –ø–æ–≤—Ç–æ—Ä—è—é—â–∏–µ—Å—è —Ñ—Ä–∞–∑—ã
        lines = response.split('\n')
        unique_lines = []
        for line in lines:
            line = line.strip()
            if line and line not in unique_lines:
                unique_lines.append(line)
        
        response = '\n'.join(unique_lines)
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –¥–ª–∏–Ω—É
        if len(response) > 500:
            response = response[:500] + "..."
        
        # –î–æ–±–∞–≤–ª—è–µ–º —ç–º–æ–¥–∑–∏ –µ—Å–ª–∏ –∏—Ö –Ω–µ—Ç
        if not any(emoji in response for emoji in ['üòä', 'üíï', 'üòò', 'üòç', 'ü•∞', 'üòâ', 'üòã']):
            response += " üòä"
        
        return response
    
    def chat_with_model(
        self,
        model_name: str,
        messages: List[Dict[str, str]],
        system_prompt: str = None
    ) -> str:
        """–ß–∞—Ç —Å –º–æ–¥–µ–ª—å—é –∏—Å–ø–æ–ª—å–∑—É—è chat API"""
        try:
            response = self.client.chat(
                model=model_name,
                messages=messages,
                options={
                    "temperature": 0.8,
                    "top_p": 0.9,
                    "num_predict": 300,
                    "repeat_penalty": 1.1
                }
            )
            
            if response and hasattr(response, 'message') and hasattr(response.message, 'content'):
                return response.message.content.strip()
            else:
                logger.error(f"–ù–µ–æ–∂–∏–¥–∞–Ω–Ω—ã–π –æ—Ç–≤–µ—Ç –æ—Ç Ollama chat: {response}")
                return "–ò–∑–≤–∏–Ω–∏—Ç–µ, –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –µ—â–µ —Ä–∞–∑."
                
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —á–∞—Ç–∞ —Å –º–æ–¥–µ–ª—å—é: {e}")
            return "–ò–∑–≤–∏–Ω–∏—Ç–µ, –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –µ—â–µ —Ä–∞–∑."
    
    def stream_generate(
        self,
        model_name: str,
        prompt: str,
        system_prompt: str = None
    ):
        """–ü–æ—Ç–æ–∫–æ–≤–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞"""
        try:
            for chunk in self.client.generate(
                model=model_name,
                prompt=prompt,
                system=system_prompt,
                stream=True,
                options={
                    "temperature": 0.8,
                    "top_p": 0.9,
                    "num_predict": 300,
                    "repeat_penalty": 1.1
                }
            ):
                if hasattr(chunk, 'response'):
                    yield chunk.response
                    
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø–æ—Ç–æ–∫–æ–≤–æ–π –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏: {e}")
            yield "–ò–∑–≤–∏–Ω–∏—Ç–µ, –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –µ—â–µ —Ä–∞–∑."
